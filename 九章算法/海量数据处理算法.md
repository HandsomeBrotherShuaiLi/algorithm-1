# 课程大纲和前言	

Lintcode 793.[Intersection of Arrays]() 求数组A和数组B的交集

这个问题只是一道很普通算法问题，我们可以通过Hash 或者排序 + 二分法等方法轻松解决。但是你以为面试就到此为止了吗，面试官马上跟进的问题，可能会让你措手不及：求两个超大文件中URLs 的交集，并且内存中不足以放下所有的URLs。这就是一个典型的海量数据处理问题。

所谓海量数据处理，其实就是基于海量数据的存储、删除、搜索等操作。所谓海量，就是数据量太大，所以导致要么无法在短时间内迅速处理，要么无法一次性装入内存。

针对时间，我们可以采用更加精妙而迅速的数据结构和算法，比如BloomFilter、Hash、堆、Bitmap等；针对空间，无非就是：大而化小，分而治之。在这里我们先不一一展开。

在海量数据处理类的问题中，我们总结了以下考点：
算法方面:
    外排序算法（External Sorting）
    Map Reduce
    非精确算法
    概率算法
    哈希算法与哈希函数（Hash Function）
数据结构方面:
    哈希表（Hash Table）
    堆（Heap）
    布隆过滤器（BloomFilter）
    位图（Bitmap）


Map Reduce

那这样的Map reduce系统有什么好处呢？
其实Map Reduce 并没有结余实际上的计算时间总和，但是如果你现在有很多的计算资源（很多台机器），你可以通过 Map Reduce 的框架利用多台机器同时计算，来优化性能进行提速。Map Reduce是一套通用的分布式计算框架。这样，对于很多类似的问题，工程师并不需要每次都去自己构思如何使用多台机器优化计算的算法，只需要套用这个通用框架，就可以快速的解决问题。（比如：矩阵分解问题，Page Rank搜索排序算法）


你可能会有疑问，为什么一定要使用Map reduce来分割文件呢，单纯的分割文件分别统计是否可行呢？
其实是不行的。单纯的将文件1丢给机器1，文件2丢给机器2，分别统计 Top K 之后再合并，这种方法是不行的。因为最高频的那一项可能分别出现在文件1和文件2，这样就相当于降低了其出现的频率，可能造成统计结果不对。

map reduce 简介

MapReduce框架流程

MapReduce的使用

MapReduce传输整理的实现

MapReduce应用练习一

MapReduce应用练习二

MapReduce的设计


# 第2章	最高频K 项问题 Top K Frequent Elements
    问题分析
    最高频K 项的离线算法
    标准离线算法的提速
    标准离线算法的空间优化
    最高频 K 项的在线算法
    标准在线算法的空间优化
    相关面试题
        最近7天的热门歌曲
        访问 Baidu 次数最多的 IP


这一类问题在海量数据类面试题中出现频率最高。问题的形式通常如下: 找到一个大文件或者数据流中出现频率最高的 K 项

这个问题的难点在于，如果条件不一样，解决的办法是完全不一样的，比如：
- 是否需要精确的 Top K 结果？即，是否允许小概率出错。
- 数据是离线的还是在线的？即是一个大文件的形式计算一次得到一个结果，还是数据流的形式实时返回结果。

接下来我们来看一个比较简单的算法题，一来让你切身地感受条件不同、解法不同的现象，更重要地是为之后解决最高频K 项问题做一个铺垫。
在一个整数数组中，找最大的 K 个整数
这个问题可以分为离线和在线两种，两种类型的解法完全不同。

lintcode 544.[]() 离线问题

这个问题，我们也可以直接使用 Quick Select 算法（参考资料），在 O(N) 的时间内找到数组的第K大的数。然后对前k大的数进行排序，时间复杂度是O(n + k log k)。两种算法都是基于快速排序算法的，十分值得学习。

QuickSelect

lintcode 545.[]() 在线的问题

这里我们再来回顾一下这道题的核心：使用堆来保存当前时刻top k的元素。我们的处理主要基于以下几点：
- 对于每一个中间时刻，前面的处理中，已经不属于top k的数据，在后续的处理中，也不可能进入top k的行列，因此没有保存的必要。
- 在已经求出前面所有数据top k项的前提下，加入一个新的数据，只需要将新数据与原来top k的数据中最小的元素比较就可以了，如果新来的元素更大，则原来top k的数据中最小的元素被剔除，将新来的元素加入top k的行列；否则保持原来top k的数据不变。
- 根据前面的分析，保存固定数量的k个数据，每次选出最小的元素，并支持添加和删除最小的元素的操作，数据结构选取最小堆最合适。

有了最大K项 问题的铺垫之后，接下来我们求解最高频K 项 的问题，就有了直观的解法：


这个算法我们暂且称之为 标准离线算法。主要的使用了两个数据结构：哈希表和最小堆。
我们来分析一下这个算法的时空复杂度：第一步统计所有单词的出现次数，需要 O(N) 的空间和 O(N) 的时间。第二步需要 O(K) 的空间和 O(NlogK) 的时间。总的时间耗费是 O(N log K)，空间耗费是 O(N)。
上面我们给出了求top K数据的标准离线算法，假如现在摆在你面前的是上 T（TrillionByte）的数据，就算是全部扫描一遍也是非常漫长的。所以虽然时间复杂度O (N log k)已经是理论下限了。但是是不是仍然可以加速呢？要解决这一问题，就要用到我们之前所讲过的map reduce了。
上一章的课程中，我们已经很详细地介绍了map reduce的原理以及优势，接下来我们就来看一下，如何在top k 的问题中，使用map reduce系统呢。

lintcode 549.[]()
使用步骤：
- 通过 Map 步骤，将每一个文件中的单词一个个取出，每个单词构造一个 <Word, 1> 的 Key-value 二元组，作为 Map 的输出。
- 通过 Reduce 的步骤，每个 Reducer（Reducer是处理reduce的机器） 会处理若干个不同的 Key，在每个 Reducer 一开始初始化的时候，构建一个最小堆（如最开始我们提到的算法），Reducer 在每次 Reduce 操作的时候，输入是 key（某个 word） 和他对应的values，其实这里我们可以假设 values 就是一堆 1（事实上 Map Reduce 会帮你做一些优化，导致有可能 value 已经被加过，所以实际处理的时候，还是老老实实的把 values 加起来，而不是看一下 values 有多少个）。那么我们把所有的 values 加起来就是当前这个 key（某个 word）的出现次数。那么当我们拿到这个单词的出现次数之后，就可以在当前的 Reducer 里去和最小堆里的第K大比大小，来决定是否淘汰当前的第K大了。Reducer 在处理完他需要处理的数据之后，就输出他得到的 Top K。
- 由于可能有多个 Reducers（跟你同时运行的机器数有关，当然一台机器也可能会运行多个Reducer），因此我们会得到多个 Top K，最后还需要从这些输出中过一遍，得到最终的 Top K。这个步骤已经在 Map Reduce 之外了，用一个单独的代码扫一遍就可以了。

在标准离线算法中，我们花费了O(N)的空间消耗，以前就是把所有的单词放入到了内存中。但在现实场景中，这个做法很可能是不行的，因为N可能非常的大。即使我们使用Map reduce系统，使用多台机器，分配到每台机器的时候，仍然可能出现无法全部加载到内存的情况。

在讨论空间优化方法时，我们简化一下原来的问题，只关注存储空间：
简化问题：假设现在只有一台机器，内存为 1G，你有一个 1T 大小的文件，需要统计里面最高频的 K 个单词。在这个问题中，我们主要用到哈希算法来优化我们的空间效率。先来介绍一些什么是Hash和Hash Code。

Hash（哈希），是一个非常常见的算法（HashTable 哈希表才是数据结构，Hash是算法）。我们在《九章算法班》中对哈希算法做过详细的介绍，如果你不知道哈希算法做了什么事情，你可以假设已经存在这样一个哈希函数，这个哈希函数对于同一个 Key，会返回一个固定的，无规律的 整数值。在工程中，你通常不需要不需要自己实现哈希函数，有很多库可以直接用。你可以在网上搜索相关的你所使用语言的库的哈希函数资料。

所谓的固定的，值得是，同样的 Key，得到的结果，肯定是同样的。同一个单词不可能一会儿算出来的哈希值（Hash Code）是1，一会儿是2。
所谓的无规律，说的是，如果你给哈希函数一大堆不同的 Key 的时候，他产生的哈希值不会扎堆，分布还是比较均匀的。
另外还有一个特点是，哈希值是可能重复的，并不是一对一的。也就是两个不同的 Key，可能得到同一个哈希值。这个特性并不影响我们的计算。

在了解了哈希算法之后，我们要如何使用哈希算法来解决内存问题呢？主要分为以下三步：
- 我们只需要先将文件扫描一次，把每个单词作为 Key，算一下他的哈希值，然后模上大概 2000 - 10000 的这样一个数。之所以取这这么一个数是因为，内存的大小是 1G，那么如果将 1T 的文件分成若干个 1G 大小的小文件的话，那么理想需要 1000 个文件。因此反之，如果你将所有的单词，分成了 1000 组的话，理想状况下，每组大概就是 1G 个不同的单词。当然这是理想状况，所以实际上处理的时候，你可以分成 2000 组比较保险。10000 组当然更保险了，但是可能就没有合理利用上内存了。实际做的时候，你可以看一下分成 2000 行不行，不行的话，再放大分组数。
- 对于每个文件，分别导入内存进行处理，即使用我们最开始提到的标准离线算法 - 哈希表+最小堆。每一组文件得到一个 Top K。
- 类似于 Map Reduce 一样，我们得到了若干个 Top K，我们最后把这若干个 Top K 再合并一次就好了。


前面我们讨论了最高频k 项的离线算法，接下来我们来讨论一下最高频k 项的在线算法。
我们还是通过一个实际问题来学习：
数据流中不断流过一些单词，提供一个接口，返回当前出现过的单词中，频次最高的 Top K 个单词。数值K 在最开始便已经给出。
即：对于给定的K，是想两个接口：
    ```
    add(word)
    # 添加一个单词到集合中

    topk()
    # 返回集合中的 Top K 的高频词
    ```
一般来说，数据流（Data Stream）问题就是我们所说的在线问题。数据流问题的特点是：你没有第二次从头访问数据的机会。
因此在离线算法中，先通过哈希表（HashMap）计数，再通过堆（Heap）来统计Top K的方法就行不通了。
类似于标准离线算法，这里我们给出标准在线算法，思路是：一边计数的同时，一边比较Top K


我们来讨论一下标准在线算法的时空复杂度：
- add 的时间复杂度是 O(logK) 的，因为最坏情况下，就是 pop 掉一个单词，push进去一个新的单词。由于 hashheap 的大小最多是 K，那么复杂度为 O(logK)
- topk 的时间复杂度是 O(KlogK)。

这个算法的空间复杂度，为计数所用的哈希表的空间复杂度。为数据流中到当前时刻为止的单词总个数。


LintCode 编程题
描述
在实时数据流中找到最常使用的k个单词.
实现TopK类中的三个方法:
TopK(k), 构造方法
add(word), 增加一个新单词


前面我们讨论了标准在线算法。空间复杂度与数据流中流过的数据大小总和有关，也就是用于计数的那个哈希表的耗费。如果你需要设计一个统计 Google 热门搜索的系统，那么这个数据量是很恐怖的。根据 Google 官方公布的数据，一天有35亿次搜索，假设每条搜索记录 20 个字节，那么会耗费 70G 的空间至少。通常哈希表的耗费要比实际存储的数据大小要大几倍才能保证效率，那么可想而知这个内存耗费是多么的恐怖。
根据这么多年的发展经验，想要寻找一个在线的，精确的，省空间的 Top K 高频项算法是不可能的。正如我们在数据库中，无法同时满足 “实时性”，“可用性”和“一致性”一样。
因此我们必须损失掉一个因子，这个因子就是准确性，换而言之就是用损失精度换空间的方法。
精确性是说，比如我求得了 Top 10 的查询，那么这10个查询中，可能会存在一个查询，他的实际排名在 Top 10 之后。又或者 Top 10 的相对排名并不正确，原本第一名的跑去了第二名。幸运的是，在实际的系统应用中，人们对精确性的要求是不高的。比如你会去验证微博热门搜索中的那些搜索真的是Top 10 的热门搜索么？你也无从验证。用户是无法感知这个不精确性的。这就留给了我们优化空间的余地。

已经有很多的科研学者们研究过精度换空间的方法，并发明了一些较为成熟的算法：
Lossy Counting
Sticky Sample
Space Saving
Efficient Count
Hash Count
...
接下来我们来介绍一个比较容易掌握的，在标准在线的算法的基础上改进最小的：Hash Count。


上面这个算法中，和标准在线算法相比，唯一的区别在于，我们将原本记录所有单词的出现次数的哈希表，换成了一个根据内存大小能开多大开多大的数组。这个数组的每个下标存储了“某些”单词的出现次数。我们使用了哈希函数（hashfunc），对每个单词计算他的哈希值（hashcode），将这个值模整个hashcount数组的大小得到一个下标（index），然后用这个下标对应的计数来代表这个单词的出现次数。有了这个单词的出现次数之后，再拿去 hash heap 里进行比较就可以了。
你应该马上会发现问题，如果有两个单词，他们的 hashcode % hashcount.size 的结果相同，那么他们的计数会被叠加到一起。从而导致计算结果的不精确。比如下面这种情况，求 Top 3 的单词，目前的统计结果是 {word1: 100, word2: 99, word: 98}。这个时候来了一个新的单词 word4，word4从没出现过，计数本应该是0，但是很巧的是，他的 hashcode % hashcount.size 的结果和 word1 是一样的。那么他就会把 word1 的计数当作是自己的计数，从而得到 100 + 1 = 101，成为 Top 1 的单词，并且挤掉了本应在 Top 3 的 word3。
上述问题对精度的影响到底有多大呢？事实上，根据“长尾效应”（Google 一下），在实际数据的统计中，由于 Top K 的 K 相对于整个数据流集合中的不同数据项个数 N 的关系是 K 远远小于 N，而 Top K 的这些数据项的计数又远远大于其他的数据项。因此，Top K 的 hashcode % hashcount.size 扎堆的可能性是非常非常小的。因此这个算法的精确度也就并不会太差。
在详细地学习了最高频K 项问题的解答方法之后，让我们用几个面试真题来实战演练一下吧。
No.1 离地球最近的 K 颗星星
给你 N 个星星的位置坐标，找到离地球最近的 M 颗星星。


LintCode 编程题
描述
给定一些 points 和一个 origin，从 points 中找到 k 个离 origin 最近的点。按照距离由小到…


No.2 最近7天的热门歌曲
问题描述： 计一个听歌统计系统，返回用户 7 天内听的最多 10 首的歌
问题分析：
在解决这个问题之前，我们需要和面试官沟通如下的几个问题条件：
- 7天和10首歌这个数字是固定的么？有可能一会儿7天一会儿10天，一会儿10首歌一会儿8首歌么？
- 对实时性要求严格么？即，是否允许一定时间的延迟？比如一首个一分钟内被点爆，是否需要在这1分钟之内在榜单中体现出来？

澄清问题是面试中重要的一个步骤，因为上述问题的答案，稍有不同，则算法的设计，系统的设计就截然不同。
我们先做如下的合理假设：
- 7天10首歌这两个数字是固定的。
- 对实时性要求不严格，可以有1小时的误差。

* 方法1: 离线算法：
这种方法比较简单粗暴，但也非常行之有效。因为通常来说，系统都会进行一些 log。比如用户在什么时候听了什么歌曲，都会被作为一条条的log 记录下来，用于以后的大数据分析用户行为的之用。那么这个时候，我们可以每小时运行一次分析程序，来计算最近7天被听的最多的10首歌。这个分析程序则读取最近 7 天的听歌记录，用前面的 Hash + Heap 的方法进行统计即可。如果这个记录过大，需要加速的话，还可以使用 Map Reduce 来提速。

* 方法2：在线算法：
这个问题较普通的 Top K 问题的区别和难点在于，有7天这个时间窗口。这个时间窗口意味着几件事情：
    新数据来的时候，需要丢弃对应的7天前的旧数据。
    7天之内的数据，都应该按照某种带着时间标记的方式被保存下来，而不是只有一个计数。
    在线 Hash + Heap 的方法“可能”不再奏效，因为跌出前10名的歌曲，还可能在过短时间后回到前10名。而之前介绍中我们在 Heap 中保存的是前10名，跌出前10名的元素不再有机会回到前10名，则无需保存。

在“方法1”中的算法，存在着如下一些缺点：
    每小时都需要进行一次对前7天的数据统计，如果数据量很大，则工作量就很大，如果使用 Map Reduce 则会耗费很多计算资源。
    如果系统的实时性要求变高，如5分钟，则该方法很有可能不奏效，因为可能5分钟无法完成对过去7天的听歌记录的统计工作。

针对这两个缺点，这里提出一种基于桶（Bucket）的统计方法：
- 聚合（Aggregate）：将用户的同个记录，按照1小时为单位进行一次聚合（Aggregate），即整合成一个 Hash 表，Key是歌曲的id，Value是歌曲在这1小时之内被播放的次数。这种方法的好处在于，因为很多歌曲，特别是热门歌曲，是被高频率点播的，这个时候没有必要去一条一条的记录点播记录，只需要记录一个1小时的统计即可。这里每个小时就是一个桶（Bucket），比如当前时刻是 1月1日的18点，那么18点之后，19点之前的点播记录，都放在18点的这个桶里，进行聚合统计。
- 滑动窗口（Sliding Window）：7天的话，只需要在内存中保存 7 * 24 = 144 个桶，随着时间轴的推移，旧的桶则可以被删除。每次需要获得 Top 10 的时候，则将这 144 个桶的结果进行合并即可。

这种方法的好处在于，如果我们对这个实时性的要求提高了，如提高到了5分钟，则把桶的大小缩小到5分钟即可。


在实际问题中，算法的实现过程中，还可能遇到其它一些问题，也是面试中可能被追问的。
Q1: 如果我们对内存要求很高，每个桶统计所用的哈希表太大，无法放进内存怎么办？
A1：这种情况下，我们必须允许一定程度的结果误差，在每个桶的局部统计中，我们可以删除那些 value 很小的 key。因为根据长尾理论，这些很冷门的歌曲，事实上占据了很大部分的内存空间，而他们最终也不会成为 Top 10 的热门歌曲。
Q2: 桶存在哪儿？如果是内存的话，断电了怎么办？
A2: 桶存同时存在内存和硬盘中。存在内存中的目的，是为了更快的计算 Top 10，存在硬盘中的目的，是断电之后可以重新快速 load 桶的内容进来。另外还有一个保险机制是，用户点播的log依然会存在数据库中，即便桶没有被存储下来，也可以通过这些点播的log重新还原每一个桶里的Hash 表。
Q3: 是每次用户想要查看 Top 10 的时候，都进行一次统计么？
A3: 不是，这个统计只会每小时进行一次。一旦统计结果获得了，就可以存在数据库里，并 cache 在内存中供用户读取。
Q4: 如果实时性要求高，比如需要精确到秒为单位，那样桶所起到的优化效果就很小了，这个时候如何快速获得最近7天的Top10？
A4: 如果是这样，则必须要有很大的内存。在 Hash + Heap 的方法中，Hash 和 Heap 都保存下所有的歌曲和其对应的点播次数。随着时间的推移，一些歌曲点播次数增加，一些歌曲的点播次数减少，然后点播次数有变化的歌曲，都在 Heap 中调整其相应的位置。然后需要获得 Top 10 时，则从 Heap 中获得。


No.3 访问 Baidu 次数最多的 IP
问题描述: 给你海量的日志数据，提取出访问百度次数最多的那个 IP。（为了简化问题，我们假设从每一行日志数据中提取 IP 的工具已经有了）
假设内存 < 4G
问题分析: 首先问题说的是 IP 地址，IP 地址的范围转换为整数是在 0 ~ 2^32-1 的。但是问题中也给出了，内存大小 < 4G，因此如果我们需要开辟一个 2^32 的整数数组，是不够的。

初步解决方法
    解决这种内存不够的大数据处理问题，通常的办法是：内存不够，就分批次处理。假如内存现在是 2G 的话，可以将这些日志数据分两批处理，IP地址的二进制表示中，先末尾是0的，再统计末尾是1的。各自保存下一个访问次数最多的 IP 地址，最后两者中取较大值。这种方法的缺点需要多次扫描文件。有一些网上的解答中，会说先把文件分割成多个文件之后再进行处理。虽然算法本质上没错，但是实现的时候不能这么实现，因为实现文件分割会产生“写”操作，写操作是比读操作慢很多的，而且也没有必要。所以读两次文件即可，而不需要真的把文件给拆分成小文件。
进一步 Follow up：如何避免多次扫描
    上面这个方法很容易想到，但是缺点也很明显。就是需要多次扫描日志文件。当这个日志文件记录非常非常大的时候，每一次扫描都是很大的耗费。有没有只扫描一次就能找到最大出现次数IP的方法呢？会想我们在这一章节中学过的最高频 K 项的在线算法。事实上，这就是一个 K = 1 的特殊情况。而在之前的解决方法中，我们也提到过，可以用 Hash Count 的算法，在极小概率会损失准确性的情况下来获得空间的优化。所以那些能够节省空间的高频项统计算法都是可以解决该问题的。唯一缺点是，有可能会损失准确性，即有可能求出的最高频项不是真正的最高项，但是这个概率很低，就看实际应用场景中对准确性的要求了。
另外也可以使用升级版的 Hash Count，即布隆过滤器（Bloom Filter）来进行统计。一般在面试中说出这个数据结构是有加分的。布隆过滤器的介绍，请看下一章节。

No.4 在 1B 个数中找出最小的 1M 个数
问题描述: Amazon：在 10 亿个数中找最小的 100 万个数。假设内存只能放下 100 万个数。题目来源
解析: 使用一个最大堆（Max Heap），保存最小的前 100 万个数。循环每个数的过程中，和 Max Heap 的堆顶比较，看看是否能被加入最小前 100 万个数里。


# 第3章	布隆过滤器（Bloom Filter）	
    标准布隆过滤器（Standard Bloom Filter）
    计数布隆过滤器 Counting Bloom Filter


在前面的内容中，HashMap是我们使用最多的数据结构，并且我们知道了在海量数据处理中，内存资源非常珍贵。布隆过滤器（Bloom Filter，简写为BF）对普通的哈希表做了进一步的改进，是一种更省空间的哈希表。当碰到内存不够的问题时，BF就是一个很好的选择。

BF一般有两个功能：
- 检测一个元素在不在一个集合中
- 统计一个元素的出现次数

在Java里面，这不就是HashSet 和 HashMap 的作用么？实际上BF能做的事情就是哈希表能做的事情，但是BF 相比哈希表，耗费更少的存储空间。既然节省了空间，同样也有一个副作用：存在 False Positive（正误识）。
什么是 False Positive? 简单的说就是，如果是 Hash 的话，他说这个元素在集合里，那就是在集合里。而如果是 BF 的话，他说在集合里，你别当真，有一定概率这个元素不在集合里。也就是说 BF 会给出一个错误的（False）肯定（Positive）。
现在你可能会有疑问，BF 会产生 False Positive 的话，是否会产生 False Negative 呢？答案是不会。也就是说，BF 说这个元素不在集合里，那就一定不在集合里。
现在你已经对BloomFilter有了大概的印象，一个更完整的BloomFilter会包含以下两个部分：
- k 个完全独立的哈希函数
- 一个很大的数组
    然后根据处理的问题的不同，BloomFilter可以分为：
- 标准型布隆过滤器（Standard Bloom Filter，简写为 SBF，对应到 Java 里的 HashSet）
- 计数型布隆过滤器（Counting Bloom Filter，简写为 CBF，对应到 Java 里的 HashMap）
    对于 SBF，其包含的大数组的类型为 Boolean 类型。对于 CBF，其包含的大数组的类型为整数类型。

接下来我们来讨论BloomFilter的第一个部分：k个不同的哈希函数
一般来说，可以使用几个不同的算法，来获得不同的哈希函数。一个比较通用的哈希函数的写法是这样：
```python
def hashfunc(string, hashsize):
    code = 0
    for c in string:
        code = code * 31 + ord(c)
        code = code % hashsize

    return code
```
想要进一步了解Hash Function的计算过程，可以看一下面这道练习题哦。


LintCode 编程题
描述: 在数据结构中，哈希函数是用来将一个字符串（或任何其他类型）转化为小于哈希表大小且大于等于零的整数。一个好的哈希函数可以尽可能少地产生冲突。一种广泛使用的哈希函数算法是使用数值33，假设任何字符串都是基于33的一个大整数，比如：

hashcode("abcd") = (ascii(a) * 333 + ascii(b) * 332 + ascii(c) *33 + ascii(d)) % HASH_SIZE 
                = (97* 333 + 98 * 332 + 99 * 33 +100) % HASH_SIZE


如果需要设计 k 个独立的哈希函数，只需要对上面的代码块，简单地修改上面的函数中的 Magic Number 31 即可，比如换成 37，41 这样。
这里你可能会有疑问，Magic Number 31的意义在哪里？
其实上面的这个算法，相当于把一个字符串当做了 31 进制，然后转换为整数。一遍转换的过程中一遍对 hashsize 取模，避免溢出。
而31也不是唯一的选择，但是有一些基本的法则我们需要遵守：
- 不能太小。太小的话，容易出现 hashfunc 算出来的值在字符串比较短的时候出现扎堆的情况。增加了哈希碰撞的几率。
- 不能太大。太大的话，影响了计算效率。
- 尽量不要是合数。合数也可能会增加哈希碰撞的几率。

了解了Hash Function的工作之后，我们会分别介绍标准型布隆过滤器（Standard Bloom Filter）和计数型布隆过滤器（Counting Bloom Filter）。

标准布隆过滤器的作用相当于一个 HashSet，即提供了这样一个数据结构，他支持如下操作：
- 在集合中加入一个元素
- 判断一个元素是否在集合中（允许 False Positive）。

其实现通常包含以下几个部分：
- 初始化：开一个足够大的 boolean 数组，初始值都是 false。
- 插入一个元素：通过 k 个哈希函数，计算出元素的 k 个哈希值，对 boolean 数组的长度取模之后，标记对应的 k 个位置上的值为 true。
- 查询一个元素：通过同样的 k 哈希函数，在 boolean 数组中取出对应的 k 个位置上的值。如果都是 true，则表示该元素可能存在，如果有一个 false，则表示一定不存在。

lintcode 556 - SBF

在具体实现的时候，为了更好地节省空间，可以用位运算的方式来代替boolean数组。Java中可以直接用BitSet这个结构。

前面我们讲了标准的BloomFilter，如果我们对SBF稍作改动，把存储所用的 boolean 数组改为 int 数组，就成为了可以计数的 BloomFilter—— Counting Bloom Filter（简写为CBF）。这个数据结构类似 Java 中的 HashMap，但只能用作计数。提供如下的几种操作：
- O(1)时间内，在集合中加入一个元素
- O(1)时间内，统计某个元素在该集合中出现的次数 - 但是可能会比实际出现次数要大一些

具体的实现步骤主要包含一下基本部分：
- 初始化：开一个足够大的 int 数组，初始值都是 0。
- 插入一个元素：通过 k 个哈希函数，计算出元素的 k 个哈希值，对 int 数组的长度取模之后，将对应的 k 个位置上的值都加一。
- 查询一个元素的出现次数：通过同样的 k 哈希函数，在 int 数组中取出对应的 k 个位置上的值。并取其中的最小值来作为该元素的出现次数预估。

lintcode - CBF


# 第4章	外排序算法	
    外排序算法的基本步骤
    相关面试题

特殊的排序算法 —— 外排序算法

外排序算法是指能够处理极大量数据的排序算法。通常来说，外排序处理的数据不能一次装入内存，只能放在读写较慢的外存储器（通常是硬盘）上。通常采用“排序-归并”的策略，将原本的大文件，拆分为若干个小文件，小文件可以读入内存中进行排序，然后使用归并操作。

因此，外排序通常分为两个基本步骤：
- 将大文件切分为若干个个小文件，并分别使用内存排好序
- 使用K路归并算法将若干个排好序的小文件合并到一个大文件中

接下来我们更具体的讨论两个步骤：
- 第一步：文件拆分：
    根据内存的大小，尽可能多的分批次的将数据 Load 到内存中，并使用系统自带的内存排序函数（或者自己写个快速排序算法），将其排好序，并输出到一个个小文件中。比如一个文件有1T，内存有1G，那么我们就这个大文件中的内容按照 1G 的大小，分批次的导入内存，排序之后输出得到 1024 个 1G 的小文件
- 第二步：K路归并排序
    在完成了大文件的拆分，并对拆分出来的小文件分别进行了排序之后，就使用K路归并算法合并排序好的文件。K路归并算法使用的是数据结构堆（Heap）来完成的，使用 Java 或者 C++ 的同学可以直接用语言自带的 PriorityQueue（C++中叫priority_queue）来代替。我们将 K 个文件中的第一个元素加入到堆里，假设数据是从小到大排序的话，那么这个堆是一个最小堆（Min Heap）。每次从堆中选出最小的元素，输出到目标结果文件中，然后如果这个元素来自第 x 个文件，则从第 x 个文件中继续读入一个新的数进来放到堆里，并重复上述操作，直到所有元素都被输出到目标结果文件中。

看到这里，你可能会发现问题，就是在归并的过程中，一个个从文件中读入数据，一个个输出到目标文件中操作很慢，如何优化？

确实，如果我们每个文件只读入1个元素并放入堆里的话，总共只用到了 1024 个元素，这很小，没有充分的利用好内存。另外，单个读入和单个输出的方式也不是磁盘的高效使用方式。因此我们可以为输入和输出都分别加入一个缓冲（Buffer）。假如一个元素有10个字节大小的话，1024 个元素一共 10K，1G的内存可以支持约 100K 组这样的数据，那么我们就为每个文件设置一个 100K 大小的 Buffer，每次需要从某个文件中读数据，都将这个 Buffer 装满。当然 Buffer 中的数据都用完的时候，再批量的从文件中读入。输出同理，设置一个 Buffer 来避免单个输出带来的效率缓慢。

在了解了外排序的过程之后，你是不是已经迫不及待地想要动手实现一下其中的一些算法了呢。先来和老师一起，实现合并 K 个有序数组的算法吧。

lintcode 486 
在一起完成了合并k个有序数组的任务之后，相信对于合并k个有序链表的问题，你一定能独自解决了。

merge K sorted List


在学习了这么多有关海量数据处理的知识之后。现在我们来尝试解决一个前面提到过的问题：求两个超大文件中 URLs 的交集。

问题的具体描述如下：
给定A、B两个文件，各存放50亿个URLs，每个 URL 各占 64 字节，内存限制是 4G，让你找出A、B文件共同的 URLs？
和之前讲过的题目一样，遇到问题首先还是要看有没有条件需要澄清，这里主要是一个问题：这两个文件各自是否已经没有重复？
对于这个问题，通常面试官会先让你假设没有重复，然后再来看有重复的情况怎么处理。那我们就先来看没有重复的情况。

- 方法1：文件拆分 Sharding（也可以叫 Partitioning）
    首先能想到的最简单的方法，肯定就是要把文件从拆分。50亿，每个 URLs 64 字节，也就是 320G 大小的文件。很显然我们不能直接全部 Load 到内存中去处理。这种内存不够的问题，通常我们的解决方法都可以是使用 hash function 来将大文件拆分为若干个小文件。比如按照hashfunc(url) % 200进行拆分的话，可以拆分成为，200 个小文件 —— 也就是如果 hashfunc(url) % 200 = 1 就把这个 url 放到 1 号文件里。每个小文件理想状况下，大小约是 1.6 G，完全可以 Load 到内存里。

    这种方法的好处在于，因为我们的目标是要去重，那么那些A和B中重复的 URLs，会被hashfunc(url) % 200映射到同一个文件中。这样在这个小文件中，来自 A 和 B 的 URls 在理想状况下一共 3.2G，可以全部导入内存进入重复判断筛选出有重复的 URLs。
    
    前面说的是理想情况，那么特殊情况下，如果 hashfunc(url) % 200 的结果比较集中，就有可能会造成不同的 URLs 在同一个文件中扎堆的情况，应该如何处理呢？
    
    这种情况下，有一些文件的大小可能会超过 4G。对于这种情况，处理的办法是进行二次拆分，把这些仍然比较大的小文件，用一个新的 hashfunc 进行拆分：hashfunc'(url) % X。这里再拆成多少个文件，可以根据文件的实际大小来定。如果二次拆分之后还是存在很大的文件，就进行三次拆分。直到每个小文件都小于 4G。
- 方法2：BloomFilter：
    既然是内存空间太少的问题，我们前面讲过了一个主要用于内存过少的情况的数据结构：BloomFilter。我们可以使用一个 4G 的 Bloom Filter，它大概包含 320 亿 个 bit。把 A 文件的 50亿 个 URLs 丢入 BF 中，然后查询 B 文件的 每个 URL 是否在 BF 里。这种方法的缺点在于，320 亿个 bit 的 BF 里存 50 亿个 URLs 实在是太满了（要考虑到BF可能会用4个哈希函数），错误率会很高。因此仍然还需需要方法1中的文件拆分来分批处理。
- 方法3：外排序算法：
    将A,B文件分别拆分为80个小文件，每个小文件4G。每个文件在拆分的时候，每4G的数据在内存中做快速排序并将有序的URLs输出到小文件中。
    用多路归并算法，将这160个小文件进行归并，在归并的过程中，即可知道哪些是重复的 URLs。只需将重复的 URLs 记录下来即可。

那么，如果A，B各自有重复的URLs怎么处理呢？
当 A, B 各自有重复的 URLs 的时候，比如最坏情况下，A里的50亿个URLs 全部一样。B里也是。这样采用方法1这种比较容易想到的 Sharding 方法，是不奏效的，因为所有 URLs 的 hashcode 都一样，就算换不同的 hashfunc 也一样。这种情况下，需要先对两个文件进行单独的去重，方法是每 4G 的数据，放到内存中用简单的哈希表进行去重。这样，在最坏情况下，总共 320G 的数据里，一个 URLs 最多重复 80次，则不会出现太严重的扎堆情况了。算法上唯一需要稍微改动的地方是，由于 A 存在多个重复的 URLs，所以当和 B 的 URLs 被sharding 到同一个文件里的时候，需要标记一下这个 URLs 来自哪个文件，这样才能知道是否在A和B中同时出现过。
另外，使用外排序的方法，是无需对两个文件进行单独去重的步骤的。


# 第5章	概率类的大数据问题

本章我们来讲另一类大数据问题：概率类的大数据问题。概率类的大数据问题，本质上是概率问题而不是大数据问题。这类问题中最常出现的则是如何在数据流中等概率的取出 M 个元素。这个问题有标准解法的，知道就知道，不知道一般也很难想到。记住就好。


问题描述：给你一个 Google 搜索日志记录，存有上亿挑搜索记录（Query）。这些搜索记录包含不同的语言。随机挑选出其中的 100 万条中文搜索记录。假设判断一条 Query 是不是中文的工具已经写好了。
这个题是一个经典的概率算法问题。这个问题的本质是一个数据流问题，虽然题目跟你说的是给了你一个“死”文件，但如果你的算法是基于 Offline 的数据的话，面试官也一定会追问一个 Online 的算法，即如何在一条一条的搜索记录飞驰而过的过程中，随机挑选出 100 万条中文搜索记录。考虑Online的另一个好处是，如果你的思路朝着离线算法的方向想的话，比如统计一下一共有 M 条记录，那么要从中挑选 N 条，就是用 N/M 的概率去挑选。那这个时候是很难处理非中文的 Queries 的。如果你的算法是一个在线算法的话，处理非中文的 Queries 就很简单，直接跳过就行了。

接下来我们就来讲一下在线的算法，这个方法你只要记住就可以了。
假设你一共要挑选 N 个 Queries，设置一个 N 的 Buffer，用于存放你选中的 Queries。对于每一条飞驰而过的Query，按照如下步骤执行你的算法：
- 如果非中文，直接跳过
- 如果 Buffer 不满，将这条 Query 直接加入 Buffer 中
- 如果 Buffer 满了，假设当前一共出了过 M 条中文 Queries，用一个随机函数，以 N / M 的概率来决定这条 Query 是否能被选中留下。
    如果没有选中，则跳过该 Query，继续处理下一条 Query
    如果选中了，则用一个随机函数，以 1 / N 的概率从 Buffer 中随机挑选一个 Query 来丢掉，让当前的 Query 放进去。

虽然已经告诉了你只要记忆就可以了，但是你可能还是疑惑，为什么这样做就可以实现等概率地抽取，那接下来我们就证明一下好了。

为了简化证明过程，我们用 5 条 Queries 里挑 3 条来作为例子证明每条 Query 被挑中的概率都是 3/5。
- 依次处理每条 Query，前 3 条 Queries 直接进入 Buffer => [1,2,3]，此时前 3 条 Queries 被选中的概率 100%
- 第 4 条 Query 处理时，有 3/4 的概率被留下，那么第 4 条 Query 被选中的概率此时就是 3/4。
- 第 4 条 Query 处理时，如果留下之后，会从 Buffer 中以 1/3 的概率踢走一条 Query。那么这些在 Buffer 中留下包含两种情况：一种是第 4 条 Query 没有被选中，概率为1/4；第二种是第 4 条 Query被选中的条件下，没有被踢走，概率是3/4 * 2/3。所以总概率是1/4 + 2/3 * 3/4 = 3/4。其中 1/4 是第 4 条 Query 没有被选中的概率。
- 综上在处理到第 4 条 Query 的时候，所有 Query 被选中的概率均为 3/4。
- 第 5 条 Query 处理时，有 3/5 的概率被留下，那么第 5 条 Query 被选中的概率此时就是 3/5。
- 第 5 条 Query 处理时，如果留下之后，会从 Buffer 中以 1/3 的概率踢走一条 Query。前4条Query能够顺利进入Buffer并被留下，首先要满足大前提，第四条Query处理后被留下来，概率为3/4。在这个大前提下，同3.分两种情况，第一种是第 5 条 Query没有被选中，概率是2/5；第二种是第 5 条 Query被选中的前提下，没有被踢走，概率是3/5 * 2/3。总概率为：：3/4 * (2/5 + 2/3 * 3/5) = 3/5。


有了前面一道题的经验，下一道题，我想你一定可以迎刃而解。
问题描述 Amazon: 一个文件中有很多行，不能全部放到内存中，如何等概率的随机挑出其中的一行？[题目来源]（https://www.careercup.com/question?id=13218749）
问题解答
先将第一行设为候选的被选中的那一行，然后一行一行的扫描文件。假如现在是第 K 行，那么第 K 行被选中踢掉现在的候选行成为新的候选行的概率为 1/K。用一个随机函数看一下是否命中这个概率即可。命中了，就替换掉现在的候选行然后继续，没有命中就继续看下一行。